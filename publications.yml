- id: pub-1
  title: 'Iterative Gaussianization: from ICA to Random Rotations'
  url: https://arxiv.org/abs/1602.00229
  date: Apr 1, 2011
  authors: Laparra et. al.
  description: Normalizing flows in the form of Gaussianization in an iterative format. Also shows connections to information theory.

- id: pub-2
  title: Non-linear Independent Components Estimation
  url: https://arxiv.org/abs/1410.8516
  date: Oct 30, 2014
  authors: Laurent Dinh, David Krueger, Yoshua Bengio.
  description: Introduces the additive coupling layer (NICE) and shows how to use it for image generation and inpainting.

- id: pub-3
  title: Masked Autoencoder for Distribution Estimation
  url: https://arxiv.org/abs/1502.03509
  date: Feb 12, 2015
  authors: Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle.
  description: |-
    Introduces MADE, a feed-forward network that uses carefully constructed binary masks on its weights to control the precise flow of information through the network. The masks ensure that each output unit receives signals only from input units that come before it in some arbitrary order. Yet all outputs can be computed in a single pass.<br>
    A popular and efficient method to bestow flows with autoregressivity is to construct them from MADE nets.
    <br><img src="assets/made.svg" alt="MADE"><br><sup>_Figure created in TikZ. [View source](https://github.com/janosh/tikz/tree/main/assets/made)._</sup>

- id: pub-4
  title: Variational Inference with Normalizing Flows
  url: https://arxiv.org/abs/1505.05770
  date: May 21, 2015
  authors: Danilo Rezende, Shakir Mohamed.
  description: They show how to go beyond mean-field variational inference by using flows to increase the flexibility of the variational family.

- id: pub-5
  title: Density estimation using Real NVP
  url: https://arxiv.org/abs/1605.08803
  date: May 27, 2016
  authors: Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio.
  description: They introduce the affine coupling layer (RNVP), a major improvement in terms of flexibility over the additive coupling layer (NICE) with unit Jacobian while keeping a single-pass forward and inverse transformation for fast sampling and density estimation, respectively.

- id: pub-6
  title: Improving Variational Inference with Inverse Autoregressive Flow
  url: https://arxiv.org/abs/1606.04934
  date: Jun 15, 2016
  authors: Diederik Kingma et al.
  description: Introduces inverse autoregressive flow (IAF), a new type of flow which scales well to high-dimensional latent spaces.

- id: pub-7
  title: Multiplicative Normalizing Flows for Variational Bayesian Neural Networks
  url: https://arxiv.org/abs/1703.01961
  date: Mar 6, 2017
  authors: Christos Louizos, Max Welling.
  description: They introduce a new type of variational Bayesian neural network that uses flows to generate auxiliary random variables which boost the flexibility of the variational family by multiplying the means of a fully-factorized Gaussian posterior over network parameters. This turns the usual diagonal covariance Gaussian into something that allows for multimodality and non-linear dependencies between network parameters.

- id: pub-8
  title: Masked Autoregressive Flow for Density Estimation
  url: https://arxiv.org/abs/1705.07057
  date: May 19, 2017
  authors: George Papamakarios, Theo Pavlakou, Iain Murray.
  description: Introduces MAF, a stack of autoregressive models forming a normalizing flow suitable for fast density estimation but slow at sampling. Analogous to Inverse Autoregressive Flow (IAF) except the forward and inverse passes are exchanged. Generalization of RNVP.

- id: pub-9
  title: Sylvester Normalizing Flow for Variational Inference
  url: https://arxiv.org/abs/1803.05649
  date: Mar 15, 2018
  authors: Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, Max Welling.
  description: Introduces Sylvester normalizing flows which remove the single-unit bottleneck from planar flows for increased flexibility in the variational posterior.

- id: pub-10
  title: Neural Autoregressive Flows
  url: https://arxiv.org/abs/1804.00779
  date: April 3, 2018
  authors: Chin-Wei Huang, David Krueger, Alexandre Lacoste, Aaron Courville.
  description: Unifies and generalize autoregressive and normalizing flow approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. Also demonstrates that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions. ([Author's Code](https://github.com/CW-Huang/NAF))

- id: pub-11
  title: Deep Density Destructors
  url: https://proceedings.mlr.press/v80/inouye18a.html
  date: July 3, 2018
  authors: Inouye & Ravikumar
  description: Normalizing flows but from an iterative perspective. Features a Tree-based density estimator.

- id: pub-12
  title: 'Glow: Generative Flow with Invertible 1x1 Convolutions'
  url: https://arxiv.org/abs/1807.03039
  date: Jul 9, 2018
  authors: Kingma, Dhariwal.
  description: They show that flows using invertible 1x1 convolution achieve high likelihood on standard generative benchmarks and can efficiently synthesize realistic-looking, large images.

- id: pub-13
  title: 'FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models'
  url: https://arxiv.org/abs/1810.01367
  date: Oct 2, 2018
  authors: Grathwohl & Chen et. al.
  description: Uses Neural ODEs as a solver to produce continuous-time normalizing flows (CNF)

- id: pub-14
  title: 'FloWaveNet : A Generative Flow for Raw Audio'
  url: https://arxiv.org/abs/1811.02155
  date: Nov 6, 2018
  authors: Kim et. al.
  description: A flow-based generative model for raw audo synthesis. ([Author's Code](https://github.com/ksw0306/FloWaveNet))

- id: pub-15
  title: Block Neural Autoregressive Flow
  url: https://arxiv.org/abs/1904.04676)
  date: Apr 9, 2019
  authors: De Cao et. al.
  description: Introduces (B-NAF), a more efficient probability density approximator. Claims to be competitive with other flows across datasets while using orders of magnitude fewer parameters.

- id: pub-16
  title: Integer Discrete Flows and Lossless Compression
  url: https://arxiv.org/abs/1905.07376
  date: May 17, 2019
  authors: Hoogeboom et. al.
  description: A normalizing flow to be used for ordinal discrete data. They introduce a flexible transformation layer called integer discrete coupling.

- id: pub-17
  title: Graph Normalizing Flows
  url: https://arxiv.org/abs/1905.13177
  date: May 30, 2019
  authors: Jenny Liu et al. A new, reversible graph network for prediction and generation.
  description: They perform similarly to message passing neural networks on supervised tasks, but at significantly reduced memory use, allowing them to scale to larger graphs. Combined with a novel graph auto-encoder for unsupervised learning, graph normalizing flows are a generative model for graph structures.

- id: pub-18
  title: Noise Regularization for Conditional Density Estimation
  url: https://arxiv.org/abs/1907.08982
  date: Jul 21, 2019
  authors: Rothfuss et. al.
  description: Normalizing flows for conditional density estimation. This paper proposes noise regularization to reduce overfitting. ([Blog](https://siboehm.com/articles/19/normalizing-flow-network) | )

- id: pub-19
  title: 'Normalizing Flows: An Introduction and Review of Current Methods'
  url: https://arxiv.org/abs/1908.09257
  date: Aug 25, 2019
  authors: Kobyzev et al.
  description: Another very thorough and very readable review article going through the basics of NFs as well as some of the state-of-the-art. Also highly recommended.

- id: pub-20
  title: Neural Spline Flows
  url: https://arxiv.org/abs/1906.04032
  date: Jun 10, 2019
  authors: Conor Durkan et. al.
  description: Uses monotonic ration splines as a coupling layer. This is currently one of the state of the art.

- id: pub-21
  title: Normalizing Flows for Probabilistic Modeling and Inference
  url: https://arxiv.org/abs/1912.02762
  date: Dec 5, 2019
  authors: Papamakarios et al.
  description: A thorough and very readable review article by some of the guys at DeepMind involved in the development of flows. Highly recommended.

- id: pub-22
  title: Invertible Generative Modeling using Linear Rational Splines
  url: https://arxiv.org/abs/2001.05168
  date: Jan 15, 2020
  authors: Dolatabadi et. al.
  description: A successor to the Neural spline flows which features an easy-to-compute inverse.

- id: pub-23
  title: Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification
  url: https://arxiv.org/abs/2001.06448
  date: Jan 17, 2020
  authors: Ardizzone et. al.
  description: They introduce a class of conditional normalizing flows with an information bottleneck objective. ([Author's Code](https://github.com/VLL-HD/exact_information_bottleneck))

- id: pub-24
  title: Stochastic Normalizing Flows
  url: https://arxiv.org/abs/2002.06707
  date: Feb 16, 2020
  authors: Hao Wu, Jonas Köhler, Frank Noé.
  description: Introduces SNF, an arbitrary sequence of deterministic invertible functions (the flow) and stochastic processes such as MCMC or Langevin Dynamics. The aim is to increase expressiveness of the chosen deterministic invertible function, while the trainable flow improves sampling efficiency over pure MCMC ([Tweet](https://twitter.com/FrankNoeBerlin/status/1229734899034329103?s=19)).

- id: pub-25
  title: Stochastic Normalizing Flows
  url: https://arxiv.org/abs/2002.09547
  date: Feb 21, 2020
  authors: Liam Hodgkinson, Chris van der Heide, Fred Roosta, Michael W. Mahoney.
  description: 'Name clash for a very different technique from the above SNF: an extension of continuous normalizing flows using stochastic differential equations (SDE). Treats Brownian motion in the SDE as a latent variable and approximates it by a flow. Aims to enable efficient training of neural SDEs which can be used for constructing efficient Markov chains.'

- id: pub-26
  title: Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows
  url: https://arxiv.org/abs/2002.10516
  date: Feb 24, 2020
  authors: Deng et. al.
  description: They propose a normalizing flow using differential deformation of the Wiener process. Applied to time series. ([Tweet](https://twitter.com/r_giaquinto/status/1309648804824723464?s=09))

- id: pub-27
  title: Gradient Boosted Normalizing Flows
  url: https://arxiv.org/abs/2002.11896
  date: Feb 27, 2020
  authors: Giaquinto & Banerjee
  description: Augment traditional normalizing flows with gradient boosting. They show that training multiple models can achieve good results and it's not necessary to have more complex distributions. ([Author's Code](https://github.com/robert-giaquinto/gradient-boosted-normalizing-flows))

- id: pub-28
  title: Gaussianization Flows
  url: https://arxiv.org/abs/2003.01941
  date: Mar 4, 2020
  authors: Meng et. al.
  description: Uses a repeated composition of trainable kernel layers and orthogonal transformations. Very competitive versus some of the SOTA like Real-NVP, Glow and FFJORD. ([Author's Code](https://github.com/chenlin9/Gaussianization_Flows))

- id: pub-29
  title: Flows for simultaneous manifold learning and density estimation
  url: https://arxiv.org/abs/2003.13913
  date: Mar 31, 2020
  authors: Brehmer & Cranmer.
  description: Normalizing flows that learn the data manifold and probability density function on that manifold. ([Tweet](https://twitter.com/kylecranmer/status/1250129080395223040?lang=es) | [Author's Code](https://github.com/johannbrehmer/manifold-flow))

- id: pub-30
  title: Normalizing Flows with Multi-Scale Autoregressive Priors
  url: https://arxiv.org/abs/2004.03891
  date: April 8, 2020
  authors: Mahajan & Bhattacharyya et. al.
  description: Improves the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR) ([Author's Code](https://github.com/visinf/mar-scf))

- id: pub-31
  title: 'Equivariant Flows: exact likelihood generative learning for symmetric densities'
  url: https://arxiv.org/abs/2006.02425
  date: Jun 3, 2020
  authors: Jonas Köhler, Leon Klein, Frank Noé.
  description: Shows that distributions generated by equivariant NFs faithfully reproduce symmetries in the underlying density. Proposes building blocks for flows which preserve typical symmetries in physical/chemical many-body systems. Shows that symmetry-preserving flows can provide better generalization and sampling efficiency.

- id: pub-32
  title: Why Normalizing Flows Fail to Detect Out-of-Distribution Data
  url: https://proceedings.neurips.cc//paper/2020/hash/ecb9fe2fbb99c31f567e9823e884dbec-Abstract.html
  date: Jun 15, 2020
  authors: Kirichenko et. al.
  description: This study how traditional normalizing flow models can suffer from out-of-distribution data. They offer a solution to combat this issue by modifying the coupling layers. ([Tweet](https://twitter.com/polkirichenko/status/1272715634544119809) | [Author's Code](https://github.com/PolinaKirichenko/flows_ood))

- id: pub-33
  title: 'SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows'
  url: https://arxiv.org/abs/2007.02731
  date: July 6, 2020
  authors: Didrik Nielsen et. al.
  description: They present a generalized framework that encompasses both Flows (deterministic maps) and VAEs (stochastic maps). By seeing deterministic maps `x = f(z)` as limiting cases of stochastic maps `x ~ p(x|z)`, the ELBO is reinterpreted as a change of variables formula for the stochastic maps. Moreover, they present a few examples of surjective layers using stochastic maps, which can be composed together with flow layers. ([Video](https://youtu.be/bXp8fk4MRXQ) | [Author's Code](https://github.com/didriknielsen/survae_flows))

- id: pub-34
  title: 'AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows'
  url: https://arxiv.org/abs/2007.07435
  date: July 15, 2020
  authors: Dolatabadi et. al.
  description: An adversarial attack method on image classifiers that use normalizing flows. ([Author's Code](https://github.com/hmdolatabadi/AdvFlow))

- id: pub-35
  title: Haar Wavelet based Block Autoregressive Flows for Trajectories
  url: https://arxiv.org/abs/2009.09878
  date: Sep 21, 2020
  authors: Bhattacharyya et. al.
  description: Introduce a Haar wavelet-based block autoregressive model.

- id: pub-36
  title: E(n) Equivariant Normalizing Flows
  url: https://arxiv.org/abs/2105.09016
  date: Jan 14, 2022
  authors: Garcia Satorras, Hoogeboom et. al.
  description: Introduces equivariant graph neural networks into the normalizing flow framework which combine to give invertible equivariant functions. Demonstrates their flow beats prior equivariant models and allows sampling of molecular configurations with positions, atom types and charges.
